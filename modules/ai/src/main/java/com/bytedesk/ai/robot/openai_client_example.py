#!/usr/bin/env python3
"""
Bytedesk OpenAI å…¼å®¹æ¥å£ç¤ºä¾‹
ä½¿ç”¨æ ‡å‡†çš„ OpenAI Python å®¢æˆ·ç«¯åº“è°ƒç”¨ Bytedesk AI æœåŠ¡

å®‰è£…ä¾èµ–:
pip install openai

ä½¿ç”¨æ–¹æ³•:
python openai_client_example.py
"""

import openai
import asyncio
from typing import List, Dict

# é…ç½® Bytedesk æœåŠ¡
BYTEDESK_BASE_URL = "http://localhost:9003/api/ai/chat/v1"
BYTEDESK_API_KEY = "your-api-key"  # å¯ä»¥æ˜¯ä»»æ„å€¼ï¼ŒBytedesk æš‚ä¸éªŒè¯


class BytedeskOpenAIClient:
    """Bytedesk OpenAI å…¼å®¹å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = BYTEDESK_BASE_URL, api_key: str = BYTEDESK_API_KEY):
        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url
        )
    
    def chat_completion(self, messages: List[Dict[str, str]], **kwargs):
        """å‘é€èŠå¤©è¯·æ±‚"""
        try:
            response = self.client.chat.completions.create(
                model="bytedesk-ai",
                messages=messages,
                **kwargs
            )
            return response
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def stream_chat_completion(self, messages: List[Dict[str, str]], **kwargs):
        """æµå¼èŠå¤©è¯·æ±‚"""
        try:
            stream = self.client.chat.completions.create(
                model="bytedesk-ai",
                messages=messages,
                stream=True,
                **kwargs
            )
            return stream
        except Exception as e:
            print(f"âŒ æµå¼è¯·æ±‚å¤±è´¥: {e}")
            return None


def test_basic_chat():
    """æµ‹è¯•åŸºæœ¬èŠå¤©åŠŸèƒ½"""
    print("ğŸ”„ æµ‹è¯•åŸºæœ¬èŠå¤©åŠŸèƒ½...")
    
    client = BytedeskOpenAIClient()
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}
    ]
    
    response = client.chat_completion(messages, temperature=0.7, max_tokens=200)
    
    if response:
        print("âœ… è¯·æ±‚æˆåŠŸ!")
        print(f"ğŸ“ å›å¤: {response.choices[0].message.content}")
        print(f"ğŸ“Š Token ä½¿ç”¨: {response.usage}")
        print(f"ğŸ†” è¯·æ±‚ ID: {response.id}")
    else:
        print("âŒ è¯·æ±‚å¤±è´¥")


def test_multi_turn_conversation():
    """æµ‹è¯•å¤šè½®å¯¹è¯"""
    print("\nğŸ”„ æµ‹è¯•å¤šè½®å¯¹è¯...")
    
    client = BytedeskOpenAIClient()
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè®°å¿†åŠ›å¾ˆå¥½çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "æˆ‘çš„åå­—æ˜¯å¼ ä¸‰ã€‚"},
        {"role": "assistant", "content": "ä½ å¥½å¼ ä¸‰ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚"},
        {"role": "user", "content": "æˆ‘çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ"}
    ]
    
    response = client.chat_completion(messages)
    
    if response:
        print("âœ… å¤šè½®å¯¹è¯æµ‹è¯•æˆåŠŸ!")
        print(f"ğŸ“ å›å¤: {response.choices[0].message.content}")
    else:
        print("âŒ å¤šè½®å¯¹è¯æµ‹è¯•å¤±è´¥")


def test_streaming_chat():
    """æµ‹è¯•æµå¼èŠå¤©"""
    print("\nğŸ”„ æµ‹è¯•æµå¼èŠå¤©...")
    
    client = BytedeskOpenAIClient()
    
    messages = [
        {"role": "user", "content": "è¯·ç»™æˆ‘è®²ä¸€ä¸ªçŸ­æ•…äº‹ã€‚"}
    ]
    
    stream = client.stream_chat_completion(messages, temperature=0.8)
    
    if stream:
        print("âœ… æµå¼è¯·æ±‚å¯åŠ¨æˆåŠŸ!")
        print("ğŸ“ æµå¼å›å¤: ", end="", flush=True)
        
        try:
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    print(chunk.choices[0].delta.content, end="", flush=True)
            print("\nğŸ æµå¼å›å¤å®Œæˆ")
        except Exception as e:
            print(f"\nâŒ æµå¼å¤„ç†é”™è¯¯: {e}")
    else:
        print("âŒ æµå¼è¯·æ±‚å¤±è´¥")


def test_different_parameters():
    """æµ‹è¯•ä¸åŒçš„å‚æ•°é…ç½®"""
    print("\nğŸ”„ æµ‹è¯•ä¸åŒå‚æ•°é…ç½®...")
    
    client = BytedeskOpenAIClient()
    
    # æµ‹è¯•é«˜åˆ›é€ æ€§å‚æ•°
    messages = [
        {"role": "user", "content": "ç”¨ä¸€ä¸ªè¯æè¿°æ˜¥å¤©ã€‚"}
    ]
    
    print("ğŸŒ¡ï¸  é«˜åˆ›é€ æ€§ (temperature=1.5):")
    response = client.chat_completion(messages, temperature=1.5, max_tokens=50)
    if response:
        print(f"   å›å¤: {response.choices[0].message.content}")
    
    print("ğŸ§Š ä½åˆ›é€ æ€§ (temperature=0.1):")
    response = client.chat_completion(messages, temperature=0.1, max_tokens=50)
    if response:
        print(f"   å›å¤: {response.choices[0].message.content}")


def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\nğŸ”„ æµ‹è¯•é”™è¯¯å¤„ç†...")
    
    # æµ‹è¯•ç©ºæ¶ˆæ¯
    client = BytedeskOpenAIClient()
    response = client.chat_completion([])
    
    if response is None:
        print("âœ… ç©ºæ¶ˆæ¯é”™è¯¯å¤„ç†æ­£ç¡®")
    else:
        print("âš ï¸  ç©ºæ¶ˆæ¯æœªæ­£ç¡®å¤„ç†")


async def test_async_chat():
    """æµ‹è¯•å¼‚æ­¥èŠå¤© (å¦‚æœéœ€è¦)"""
    print("\nğŸ”„ æµ‹è¯•å¼‚æ­¥åŠŸèƒ½...")
    print("â„¹ï¸  å½“å‰ä½¿ç”¨åŒæ­¥å®¢æˆ·ç«¯ï¼Œå¼‚æ­¥åŠŸèƒ½éœ€è¦ AsyncOpenAI")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Bytedesk OpenAI å…¼å®¹æ¥å£æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥æœåŠ¡è¿é€šæ€§
    print("ğŸ” æ£€æŸ¥æœåŠ¡è¿é€šæ€§...")
    try:
        import requests
        response = requests.get("http://localhost:9003/actuator/health", timeout=5)
        if response.status_code == 200:
            print("âœ… Bytedesk æœåŠ¡è¿æ¥æ­£å¸¸")
        else:
            print("âš ï¸  Bytedesk æœåŠ¡çŠ¶æ€å¼‚å¸¸")
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° Bytedesk æœåŠ¡: {e}")
        print("è¯·ç¡®ä¿æœåŠ¡è¿è¡Œåœ¨ localhost:9003")
        return
    
    # è¿è¡Œæµ‹è¯•
    test_basic_chat()
    test_multi_turn_conversation()
    test_streaming_chat()
    test_different_parameters()
    test_error_handling()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("\nğŸ’¡ é›†æˆæç¤º:")
    print("1. åªéœ€è¦ä¿®æ”¹ base_url å°±å¯ä»¥ä½¿ç”¨ç°æœ‰çš„ OpenAI ä»£ç ")
    print("2. API Key å¯ä»¥æ˜¯ä»»æ„å€¼ï¼ŒBytedesk å½“å‰ä¸åšéªŒè¯")
    print("3. æ”¯æŒæ‰€æœ‰æ ‡å‡†çš„ OpenAI Chat Completions å‚æ•°")
    print("4. æ”¯æŒæµå¼å’Œéæµå¼å“åº”")


if __name__ == "__main__":
    main()
